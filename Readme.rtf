Aura

Aura is a visionOS application for real-time, on-device transcription and environmental sound awareness. It enhances user perception by overlaying live conversational text and sound alerts in both a standard 2D window and a fully immersive space, with a core focus on privacy.

Core Features

	•	On-Device Processing: Provides real-time, private audio processing by running both transcription and sound analysis directly on the device.
	•	Intelligent Text Segmentation: Automatically breaks the live transcript into readable sentences and phrases by detecting natural pauses and punctuation.
	•	Multi-Language Support: Allows users to change the speech recognition language on the fly from a pre-defined list that includes English, Spanish, Hindi, and more.
	•	Dual Interface: Offers both a 2D window panel for the shared space and a fully immersive experience where the UI floats and follows the user's gaze.

Key Technologies

	•	UI: SwiftUI and RealityKit.
	•	Audio Processing: AVFoundation for microphone input.
	•	Speech Recognition: Speech framework for live, on-device transcription.
	•	Sound Analysis: SoundAnalysis framework for environmental sound classification.
	•	State Management: @Observable for reactive state updates.

Technical Implementation

The application uses AVFoundation to capture microphone input. This audio stream is simultaneously processed by the Speech framework for live transcription and the SoundAnalysis framework for environmental sound classification.
The immersive view panel is created by rendering a SwiftUI view into a UIImage, which is then converted into a TextureResource. This texture is applied to a ModelEntity plane in RealityKit, allowing for a dynamic and responsive UI driven entirely by SwiftUI within a 3D space.

Project Setup

This project is built natively for visionOS and has no external dependencies. Simply open it in Xcode and run it on the visionOS simulator or a physical device. You will need to grant Microphone and Speech Recognition permissions on the first launch.

Limitations and Future Work

Currently, the application does not determine the direction of incoming sounds. While the underlying audio monitor has initial code for azimuth detection from stereo input, this feature is not yet exposed in the UI and is considered a primary goal for future development as hardware capabilities evolve.

Acknowledgements

This project was created as a part of a hackathon hosted by LA Tech Week, the USC Information Sciences Institute (ISI), and Lovable.
